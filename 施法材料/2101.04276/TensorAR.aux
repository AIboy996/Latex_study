\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{apalike}
\citation{chen2019factor}
\citation{chen2019modeling}
\citation{walden2002wavelet}
\citation{tucker1966some}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{eq:VAR}{{1}{2}{Introduction}{section.1}{}}
\MT@newlabel{eq:VAR}
\citation{wang2016factor}
\citation{chen2017constrained}
\citation{chen2019factor}
\citation{negahban2011estimation,basu2015regularized,han2015direct,wang2019high,zheng20}
\citation{chen2018autoregressive}
\citation{hoff15}
\citation{gandy2011tensor,tomioka2011statistical,liu2013tensor,raskutti2019convex}
\MT@newlabel{eq:VAR}
\citation{negahban2011estimation}
\citation{mu2014square}
\MT@newlabel{eq:VAR}
\citation{DingCook18}
\citation{raskutti2019convex}
\citation{chen2019non}
\citation{han2020optimal}
\citation{kolda2009tensor}
\newlabel{sec:prelim}{{2}{6}{Preliminaries: Notation and Tensor Algebra}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries: Notation and Tensor Algebra }{6}{section.2}\protected@file@percent }
\citation{tucker1966some,deLathauwer2000multilinear}
\newlabel{eq:tensorinner}{{2}{8}{Preliminaries: Notation and Tensor Algebra}{section.2}{}}
\newlabel{eq:inner_ex}{{2}{8}{Preliminaries: Notation and Tensor Algebra}{section.2}{}}
\newlabel{eq:inner_out}{{2}{8}{Preliminaries: Notation and Tensor Algebra}{section.2}{}}
\newlabel{eq:vecinner}{{2}{8}{Preliminaries: Notation and Tensor Algebra}{section.2}{}}
\newlabel{eq:Tucker}{{2}{9}{Preliminaries: Notation and Tensor Algebra}{section.2}{}}
\MT@newlabel{eq:Tucker}
\newlabel{eq:mmmat}{{2}{9}{Preliminaries: Notation and Tensor Algebra}{section.2}{}}
\MT@newlabel{eq:Tucker}
\MT@newlabel{eq:Tucker}
\newlabel{sec:LRTAR}{{3}{10}{Low-Rank Tensor Autoregression}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Low-Rank Tensor Autoregression}{10}{section.3}\protected@file@percent }
\newlabel{eq:LTRTAR_model}{{3}{10}{Low-Rank Tensor Autoregression}{section.3}{}}
\MT@newlabel{eq:tensorinner}
\MT@newlabel{eq:vecinner}
\MT@newlabel{eq:LTRTAR_model}
\MT@newlabel{eq:VAR}
\newlabel{eq:Tucker_model}{{3}{10}{Low-Rank Tensor Autoregression}{section.3}{}}
\MT@newlabel{eq:mmmat}
\MT@newlabel{eq:LTRTAR_model}
\newlabel{eq:VAR_rep}{{3}{10}{Low-Rank Tensor Autoregression}{section.3}{}}
\MT@newlabel{eq:VAR}
\MT@newlabel{eq:LTRTAR_model}
\newlabel{eq:dim}{{3}{10}{Low-Rank Tensor Autoregression}{section.3}{}}
\MT@newlabel{eq:VAR_rep}
\MT@newlabel{eq:LTRTAR_model}
\citation{chen2018autoregressive}
\citation{hoff15}
\citation{hoff15}
\newlabel{asmp:stationary}{{1}{11}{}{assumption.1}{}}
\MT@newlabel{eq:inner_ex}
\MT@newlabel{eq:inner_out}
\MT@newlabel{eq:LTRTAR_model}
\MT@newlabel{eq:Tucker_model}
\newlabel{eq:factor_model}{{3}{11}{Low-Rank Tensor Autoregression}{assumption.1}{}}
\MT@newlabel{eq:factor_model}
\MT@newlabel{eq:factor_model}
\newlabel{ex1}{{1}{11}{}{example.1}{}}
\MT@newlabel{eq:VAR_rep}
\newlabel{eq:VAR_mat}{{1}{11}{}{example.1}{}}
\MT@newlabel{eq:factor_model}
\newlabel{eq:bilinear}{{1}{11}{}{example.1}{}}
\newlabel{eq:bilinear_vec}{{1}{11}{}{example.1}{}}
\citation{chen2018autoregressive}
\citation{hoff15}
\citation{SW11,BW16}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the MAR model and the proposed LRTAR model in the case of $d=2$. }}{12}{figure.1}\protected@file@percent }
\newlabel{fg:Fig_TAR}{{1}{12}{Illustration of the MAR model and the proposed LRTAR model in the case of $d=2$}{figure.1}{}}
\MT@newlabel{eq:VAR_mat}
\MT@newlabel{eq:bilinear_vec}
\MT@newlabel{eq:bilinear}
\newlabel{eq:multilinear}{{1}{12}{}{example.1}{}}
\MT@newlabel{eq:multilinear}
\newlabel{eq:multilinear_vec}{{1}{12}{}{example.1}{}}
\MT@newlabel{eq:multilinear_vec}
\MT@newlabel{eq:VAR_rep}
\MT@newlabel{eq:dim}
\newlabel{ex2}{{2}{12}{}{example.2}{}}
\citation{chen2019factor}
\citation{shapiro1986asymptotic}
\newlabel{eq:dfm_tensor}{{2}{13}{}{example.2}{}}
\MT@newlabel{eq:dfm_tensor}
\MT@newlabel{eq:dfm_tensor}
\MT@newlabel{eq:dfm_tensor}
\newlabel{sec:LDM}{{4}{13}{Low-Dimensional Least Squares Estimation}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Low-Dimensional Least Squares Estimation }{13}{section.4}\protected@file@percent }
\newlabel{eq:LTR}{{4}{13}{Low-Dimensional Least Squares Estimation}{section.4}{}}
\MT@newlabel{eq:LTR}
\citation{shapiro1986asymptotic}
\newlabel{eq:H}{{4}{14}{Low-Dimensional Least Squares Estimation}{section.4}{}}
\newlabel{thm:Asymptotic}{{1}{14}{}{theorem.1}{}}
\MT@newlabel{eq:LTRTAR_model}
\MT@newlabel{eq:VAR_rep}
\citation{Zhou13}
\citation{li2018tucker}
\newlabel{eq:RRR}{{4}{15}{Low-Dimensional Least Squares Estimation}{theorem.1}{}}
\MT@newlabel{eq:LTRTAR_model}
\newlabel{cor:OLSRRR}{{1}{15}{}{corollary.1}{}}
\MT@newlabel{eq:LTR}
\MT@newlabel{eq:VAR_rep}
\MT@newlabel{eq:LTR}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces ALS algorithm for LTR estimator}}{16}{algorithm.1}\protected@file@percent }
\newlabel{alg:LSE}{{1}{16}{Low-Dimensional Least Squares Estimation}{algorithm.1}{}}
\MT@newlabel{eq:LTR}
\citation{gandy2011tensor,tomioka2011statistical,liu2013tensor,raskutti2019convex}
\citation{basu2015regularized,raskutti2019convex}
\newlabel{sec:HDM}{{5}{17}{High-Dimensional Regularized Estimation}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}High-Dimensional Regularized Estimation}{17}{section.5}\protected@file@percent }
\newlabel{subsec:1mode}{{5.1}{17}{Regularization via One-Mode Matricization}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Regularization via One-Mode Matricization }{17}{subsection.5.1}\protected@file@percent }
\MT@newlabel{eq:LTRTAR_model}
\MT@newlabel{eq:LTRTAR_model}
\newlabel{eq:SN_norm}{{5.1}{17}{Regularization via One-Mode Matricization}{subsection.5.1}{}}
\newlabel{asmp:gaussian}{{2}{17}{}{assumption.2}{}}
\citation{basu2015regularized}
\citation{tomioka2011statistical}
\newlabel{thm:SN}{{2}{18}{}{theorem.2}{}}
\citation{mu2014square,raskutti2019convex}
\citation{negahban2011estimation}
\MT@newlabel{eq:RRR}
\MT@newlabel{eq:VAR_rep}
\newlabel{eq:matrixnuclear}{{5.1}{19}{Regularization via One-Mode Matricization}{theorem.2}{}}
\newlabel{thm:MN}{{3}{19}{}{theorem.3}{}}
\citation{negahban2011estimation}
\citation{mu2014square}
\MT@newlabel{eq:SN_norm}
\newlabel{subsec:sqrmode}{{5.2}{21}{Regularization via Square Matricization}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Regularization via Square Matricization }{21}{subsection.5.2}\protected@file@percent }
\MT@newlabel{eq:SN_norm}
\MT@newlabel{eq:matrixnuclear}
\newlabel{eq:SSN_norm}{{5.2}{21}{Regularization via Square Matricization}{subsection.5.2}{}}
\newlabel{eq:SSN_est}{{5.2}{21}{Regularization via Square Matricization}{subsection.5.2}{}}
\citation{Han_Xu_Liu2015}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of the sample size conditions and error upper bounds in Theorems \ref  {thm:SN}--\ref  {thm:SSN}, where $p_{-i}=\DOTSB \prod@ \slimits@ _{j=1, j\neq  i}^{d}p_j$, $r_q=(2d)^{-1}\DOTSB \sum@ \slimits@ _{i=1}^{2d}r_q^{(i)}$, and $s_q=2^{1-d}\DOTSB \sum@ \slimits@ _{k=1}^{2^{d-1}}s_q^{(k)}$. }}{22}{table.1}\protected@file@percent }
\newlabel{table1}{{1}{22}{Summary of the sample size conditions and error upper bounds in Theorems \ref {thm:SN}--\ref {thm:SSN}, where $p_{-i}=\prod _{j=1, j\neq i}^{d}p_j$, $r_q=(2d)^{-1}\sum _{i=1}^{2d}r_q^{(i)}$, and $s_q=2^{1-d}\sum _{k=1}^{2^{d-1}}s_q^{(k)}$}{table.1}{}}
\MT@newlabel{eq:SN_norm}
\newlabel{thm:SSN}{{4}{22}{}{theorem.4}{}}
\newlabel{subsec:trunc}{{5.3}{23}{Truncated Regularized Estimation}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Truncated Regularized Estimation }{23}{subsection.5.3}\protected@file@percent }
\citation{negahban2011estimation}
\newlabel{asmp:truncate}{{3}{24}{}{assumption.3}{}}
\newlabel{thm:rankconsistency}{{5}{24}{}{theorem.5}{}}
\citation{gandy2011tensor}
\citation{boyd2011distributed}
\citation{gandy2011tensor}
\newlabel{alg:SSN}{{2}{25}{ADMM Algorithm}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces ADMM algorithm for (T)SSN estimator }}{25}{algorithm.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}ADMM Algorithm}{25}{subsection.5.4}\protected@file@percent }
\newlabel{subsec:ADMM}{{5.4}{25}{ADMM Algorithm}{subsection.5.4}{}}
\MT@newlabel{eq:SSN_est}
\newlabel{eq:objective}{{5.4}{25}{ADMM Algorithm}{subsection.5.4}{}}
\MT@newlabel{eq:objective}
\MT@newlabel{eq:objective}
\citation{gandy2011tensor}
\MT@newlabel{eq:objective}
\newlabel{sec:sim}{{6}{26}{Simulation Studies}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Simulation Studies }{26}{section.6}\protected@file@percent }
\MT@newlabel{eq:LTRTAR_model}
\citation{French_data}
\citation{lam2012factor}
\@writefile{toc}{\contentsline {section}{\numberline {7}Real Data Analysis}{28}{section.7}\protected@file@percent }
\newlabel{sec:realdata}{{7}{28}{Real Data Analysis}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Matrix-Valued Time Series}{28}{subsection.7.1}\protected@file@percent }
\newlabel{subsec:matrix_valued}{{7.1}{28}{Matrix-Valued Time Series}{subsection.7.1}{}}
\citation{chen2018autoregressive}
\citation{wang2016factor}
\MT@newlabel{eq:factor_model}
\citation{fama2015five}
\citation{French_data}
\citation{chen2019factor}
\newlabel{subsec:tensor_valued}{{7.2}{31}{Three-Way Tensor-Valued Time Series}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Three-Way Tensor-Valued Time Series }{31}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion and Discussion}{32}{section.8}\protected@file@percent }
\newlabel{sec:conclusion}{{8}{32}{Conclusion and Discussion}{section.8}{}}
\citation{basu2015regularized,Davis2016}
\citation{wang2019high}
\citation{chen2019modeling}
\citation{li2018tucker}
\citation{bi2018multilayer}
\citation{frandsen2019understanding}
\citation{shapiro1986asymptotic}
\citation{shapiro1986asymptotic}
\citation{shapiro1986asymptotic}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix A:}Proofs for Low-Dimensional Estimation}{35}{section.9}\protected@file@percent }
\MT@newlabel{eq:VAR_rep}
\MT@newlabel{eq:H}
\newlabel{eq:proj1}{{Appendix A:}{36}{Proofs for Low-Dimensional Estimation}{section.9}{}}
\newlabel{eq:proj2}{{Appendix A:}{36}{Proofs for Low-Dimensional Estimation}{section.9}{}}
\MT@newlabel{eq:proj1}
\MT@newlabel{eq:proj2}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix B:}Proofs for High-Dimensional Estimation}{36}{section.10}\protected@file@percent }
\newlabel{sec:prelimproof}{{B.1}{37}{Preliminary Analysis}{subsection.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Preliminary Analysis }{37}{subsection.10.1}\protected@file@percent }
\newlabel{subsec:deterministic}{{B.1.1}{37}{Deterministic Analysis}{subsubsection.10.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Deterministic Analysis }{37}{subsubsection.10.1.1}\protected@file@percent }
\newlabel{eq:subspace_N}{{B.1.1}{38}{Deterministic Analysis}{subsubsection.10.1.1}{}}
\newlabel{eq:subspace_M}{{B.1.1}{38}{Deterministic Analysis}{subsubsection.10.1.1}{}}
\MT@newlabel{eq:subspace_N}
\MT@newlabel{eq:subspace_M}
\citation{negahban2012restricted}
\citation{negahban2012unified}
\newlabel{lemma:ApproxCone}{{B.1}{39}{}{lemma.1}{}}
\newlabel{dfn:rsc}{{2}{39}{}{definition.2}{}}
\newlabel{lemma:errorbound}{{B.2}{39}{}{lemma.2}{}}
\newlabel{subsec:stochastic}{{B.1.2}{40}{Stochastic Analysis}{subsubsection.10.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}Stochastic Analysis }{40}{subsubsection.10.1.2}\protected@file@percent }
\newlabel{lemma:dualnorm}{{B.3}{40}{Deviation bound}{lemma.3}{}}
\newlabel{lemma:RSC}{{B.4}{41}{Strong convexity}{lemma.4}{}}
\newlabel{subsec:proof_theorem}{{B.2}{41}{Proofs of Theorems \ref {thm:SN}--\ref {thm:rankconsistency}}{subsection.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proofs of Theorems \ref  {thm:SN}--\ref  {thm:rankconsistency} }{41}{subsection.10.2}\protected@file@percent }
\citation{mirsky1960symmetric}
\newlabel{eq:mirsky}{{B.2}{42}{Proofs of Theorems \ref {thm:SN}--\ref {thm:rankconsistency}}{subsection.10.2}{}}
\newlabel{eq:singul}{{B.2}{42}{Proofs of Theorems \ref {thm:SN}--\ref {thm:rankconsistency}}{subsection.10.2}{}}
\MT@newlabel{eq:singul}
\MT@newlabel{eq:mirsky}
\newlabel{subsec:lemma}{{B.3}{43}{Proofs of Lemmas \ref {lemma:ApproxCone}--\ref {lemma:RSC}}{subsection.10.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Proofs of Lemmas \ref  {lemma:ApproxCone}--\ref  {lemma:RSC} }{43}{subsection.10.3}\protected@file@percent }
\citation{negahban2011estimation}
\citation{tomioka2011statistical}
\citation{basu2015regularized}
\newlabel{subsec:auxlemma}{{B.4}{48}{Three Auxiliary Lemmas}{subsection.10.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Three Auxiliary Lemmas }{48}{subsection.10.4}\protected@file@percent }
\newlabel{lemma:deviation}{{B.5}{48}{Deviation bound on different matricizations}{lemma.5}{}}
\newlabel{eq:martingale_deviation}{{B.4}{49}{Three Auxiliary Lemmas}{lemma.5}{}}
\citation{candes2011tight}
\newlabel{eq:sup_covering}{{B.4}{50}{Three Auxiliary Lemmas}{lemma.5}{}}
\MT@newlabel{eq:martingale_deviation}
\citation{basu2015regularized}
\MT@newlabel{eq:sup_covering}
\newlabel{lemma:deviation2}{{B.6}{51}{}{lemma.6}{}}
\newlabel{eq:P}{{B.6}{51}{}{lemma.6}{}}
\citation{Vershynin2018}
\MT@newlabel{eq:P}
\newlabel{lemma:covering}{{B.7}{52}{}{lemma.7}{}}
\bibdata{mybib}
\bibcite{BW16}{{1}{2016}{{Bai and Wang}}{{}}}
\bibcite{basu2015regularized}{{2}{2015}{{Basu and Michailidis}}{{}}}
\bibcite{bi2018multilayer}{{3}{2018}{{Bi et~al.}}{{}}}
\bibcite{boyd2011distributed}{{4}{2011}{{Boyd et~al.}}{{}}}
\bibcite{candes2011tight}{{5}{2011}{{Candes and Plan}}{{}}}
\bibcite{chen2019modeling}{{6}{2019}{{Chen and Chen}}{{}}}
\bibcite{chen2017constrained}{{7}{2020a}{{Chen et~al.}}{{}}}
\bibcite{chen2019non}{{8}{2019}{{Chen et~al.}}{{}}}
\bibcite{chen2018autoregressive}{{9}{2020b}{{Chen et~al.}}{{}}}
\bibcite{chen2019factor}{{10}{2020c}{{Chen et~al.}}{{}}}
\bibcite{Davis2016}{{11}{2016}{{Davis et~al.}}{{}}}
\bibcite{deLathauwer2000multilinear}{{12}{2000}{{De~Lathauwer et~al.}}{{}}}
\bibcite{DingCook18}{{13}{2018}{{Ding and Cook}}{{}}}
\bibcite{fama2015five}{{14}{2015}{{Fama and French}}{{}}}
\bibcite{frandsen2019understanding}{{15}{2019}{{Frandsen and Ge}}{{}}}
\bibcite{French_data}{{16}{2020}{{French}}{{}}}
\bibcite{gandy2011tensor}{{17}{2011}{{Gandy et~al.}}{{}}}
\bibcite{han2015direct}{{18}{2015a}{{Han et~al.}}{{}}}
\bibcite{Han_Xu_Liu2015}{{19}{2015b}{{Han et~al.}}{{}}}
\bibcite{han2020optimal}{{20}{2020}{{Han et~al.}}{{}}}
\bibcite{hoff15}{{21}{2015}{{Hoff}}{{}}}
\bibcite{kolda2009tensor}{{22}{2009}{{Kolda and Bader}}{{}}}
\bibcite{lam2012factor}{{23}{2012}{{Lam et~al.}}{{}}}
\bibcite{li2018tucker}{{24}{2018}{{Li et~al.}}{{}}}
\bibcite{liu2013tensor}{{25}{2013}{{Liu et~al.}}{{}}}
\bibcite{mirsky1960symmetric}{{26}{1960}{{Mirsky}}{{}}}
\bibcite{mu2014square}{{27}{2014}{{Mu et~al.}}{{}}}
\bibcite{negahban2011estimation}{{28}{2011}{{Negahban and Wainwright}}{{}}}
\bibcite{negahban2012restricted}{{29}{2012}{{Negahban and Wainwright}}{{}}}
\bibcite{negahban2012unified}{{30}{2012}{{Negahban et~al.}}{{}}}
\bibcite{raskutti2019convex}{{31}{2019}{{Raskutti et~al.}}{{}}}
\bibcite{shapiro1986asymptotic}{{32}{1986}{{Shapiro}}{{}}}
\bibcite{SW11}{{33}{2011}{{Stock and Watson}}{{}}}
\bibcite{tomioka2011statistical}{{34}{2011}{{Tomioka et~al.}}{{}}}
\bibcite{tucker1966some}{{35}{1966}{{Tucker}}{{}}}
\bibcite{Vershynin2018}{{36}{2018}{{Vershynin}}{{}}}
\bibcite{walden2002wavelet}{{37}{2002}{{Walden and Serroukh}}{{}}}
\bibcite{wang2016factor}{{38}{2019}{{Wang et~al.}}{{}}}
\bibcite{wang2019high}{{39}{2020}{{Wang et~al.}}{{}}}
\bibcite{zheng20}{{40}{2020}{{Zheng and Cheng}}{{}}}
\bibcite{Zhou13}{{41}{2013}{{Zhou et~al.}}{{}}}
\bibstyle{apalike}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Average estimation error of LTR, OLS and RRR estimators for data generated with different $d$, $p_i$'s and multilinear ranks.}}{57}{figure.2}\protected@file@percent }
\newlabel{fg:simulation1}{{2}{57}{Average estimation error of LTR, OLS and RRR estimators for data generated with different $d$, $p_i$'s and multilinear ranks}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parameter setting for eight cases with different varying parameters, where $\boldsymbol  {p}=(p_1,\dots  , p_d)$ and $\boldsymbol  {r}=(r_1,\dots  , r_{2d})$.}}{58}{table.2}\protected@file@percent }
\newlabel{tab1}{{2}{58}{Parameter setting for eight cases with different varying parameters, where $\bm {p}=(p_1,\dots , p_d)$ and $\bm {r}=(r_1,\dots , r_{2d})$}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Average squared estimation error for the SSN estimator for eight cases with different varying parameters. The error bars in each panel represent $\pm $ one standard deviation.}}{58}{figure.3}\protected@file@percent }
\newlabel{fg:simulation2}{{3}{58}{Average squared estimation error for the SSN estimator for eight cases with different varying parameters. The error bars in each panel represent $\pm $ one standard deviation}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Average estimation error for TSSN, SSN, MN, and SN estimators for data generated with different $d$, $p_i$'s and multilinear ranks.}}{59}{figure.4}\protected@file@percent }
\newlabel{fg:simulation3}{{4}{59}{Average estimation error for TSSN, SSN, MN, and SN estimators for data generated with different $d$, $p_i$'s and multilinear ranks}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces TSSN estimates of predictor and response factor matrices for $10\times 10$ matrix-valued portfolio return series. From left to right: $\boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_1, \boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_2, \boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_3$ and $\boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_4$.}}{60}{figure.5}\protected@file@percent }
\newlabel{fig:factor1}{{5}{60}{TSSN estimates of predictor and response factor matrices for $10\times 10$ matrix-valued portfolio return series. From left to right: $\bm {\widetilde {U}}_1, \bm {\widetilde {U}}_2, \bm {\widetilde {U}}_3$ and $\bm {\widetilde {U}}_4$}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Average in-sample forecasting error and out-of-sample rolling forecasting error for $10\times 10$ matrix-valued portfolio return series. The best cases are marked in bold.}}{60}{table.3}\protected@file@percent }
\newlabel{tbl:in-sample-error}{{3}{60}{Average in-sample forecasting error and out-of-sample rolling forecasting error for $10\times 10$ matrix-valued portfolio return series. The best cases are marked in bold}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces TSSN estimates of predictor and response factor matrices for $4\times 4\times 2$ tensor-valued portfolio return series. From left to right: $\boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_1, \boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_2, \boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_3$, $\boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_4$, $\boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_5$ and $\boldsymbol  {\setbox \z@ \hbox {\mathsurround \z@ $\textstyle U$}\mathaccent "0365{U}}_6$.}}{61}{figure.6}\protected@file@percent }
\newlabel{fig:factor2}{{6}{61}{TSSN estimates of predictor and response factor matrices for $4\times 4\times 2$ tensor-valued portfolio return series. From left to right: $\bm {\widetilde {U}}_1, \bm {\widetilde {U}}_2, \bm {\widetilde {U}}_3$, $\bm {\widetilde {U}}_4$, $\bm {\widetilde {U}}_5$ and $\bm {\widetilde {U}}_6$}{figure.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Average in-sample forecasting error and out-of-sample rolling forecasting error for $4\times 4\times 2$ tensor-valued portfolio return series. The best cases are marked in bold.}}{61}{table.4}\protected@file@percent }
\newlabel{tbl:in-sample-error2}{{4}{61}{Average in-sample forecasting error and out-of-sample rolling forecasting error for $4\times 4\times 2$ tensor-valued portfolio return series. The best cases are marked in bold}{table.4}{}}
